{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Identification based on deep neural networks and ngrams\n",
    "This approach of language identification follows the paper: Language Identification a Neural Network Approach, https://core.ac.uk/download/pdf/62918899.pdf\n",
    "\n",
    "Feature extraction is partly inspired by:\n",
    "https://github.com/conorosully/medium-articles/blob/master/src/language_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The data can be downloaded from: https://downloads.tatoeba.org/exports/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "# TODO: Justify assumptions\n",
    "MIN_LEN = 20\n",
    "MAX_LEN = 200\n",
    "\n",
    "LANG = ['deu', 'eng', 'fra']\n",
    "\n",
    "DATA_SIZE = 5000\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sentences.csv',\n",
    "                  sep='\\t',\n",
    "                  encoding='utf8',\n",
    "                  index_col=0,\n",
    "                  names=['lang', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter text by length\n",
    "filter_len = [True if MIN_LEN <= len(t) <= MAX_LEN else False for t in data['text']]\n",
    "data = data[filter_len]\n",
    "\n",
    "# Filter text by language\n",
    "filter_lang = [True if l in LANG else False for l in data['lang']]\n",
    "data = data[filter_lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and crop data\n",
    "data_sample = data.sample(n=DATA_SIZE)\n",
    "\n",
    "# Split data into test set and training set\n",
    "offset = int(TEST_SIZE * DATA_SIZE)\n",
    "data_test = data_sample[:offset]\n",
    "data_train = data_sample[offset:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ngrams for a specific language\n",
    "def get_ngrams(corpus, n, max_features):\n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                                ngram_range=(n, n),\n",
    "                                max_features=max_features)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return X, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<4000x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 7765 stored elements in Compressed Sparse Row format>,\n",
       " [' is ',\n",
       "  ' mar',\n",
       "  ' tha',\n",
       "  ' the',\n",
       "  ' to ',\n",
       "  ' tom',\n",
       "  ' was',\n",
       "  ' you',\n",
       "  'and ',\n",
       "  'ary ',\n",
       "  'hat ',\n",
       "  'ich ',\n",
       "  'ing ',\n",
       "  'mary',\n",
       "  \"n't \",\n",
       "  'that',\n",
       "  'the ',\n",
       "  'tom ',\n",
       "  'was ',\n",
       "  'you '])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = {}\n",
    "feature_names = get_ngrams(data_train['text'], 4, 20)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most frequent ngrams for every language\n",
    "def get_feature_names(data, n, max_features):\n",
    "    \n",
    "    features = set()\n",
    "        \n",
    "    # get features for every language\n",
    "    for l in LANG:\n",
    "        corpus = data[data.lang==l]['text']\n",
    "        _, ngrams = get_ngrams(corpus, n, 20)\n",
    "        features.update(ngrams)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normalized frequency matrix for specified ngrams\n",
    "def get_feature_matrix(data, n, ngrams):\n",
    "    vocab = {}\n",
    "    for i, fn in enumerate(ngrams):\n",
    "        vocab[fn]=i\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                                ngram_range=(n, n),\n",
    "                                vocabulary=vocab)\n",
    "    \n",
    "    X = vectorizer.transform(data['text'])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    feature_matrix = pd.DataFrame(data=X.toarray(), columns=feature_names)\n",
    "    \n",
    "    # normalize matrix\n",
    "    count_min = feature_matrix.min()\n",
    "    count_max = feature_matrix.max()\n",
    "    \n",
    "    feature_matrix = (feature_matrix - count_min) / (count_max - count_min)\n",
    "    \n",
    "    # add target variable\n",
    "    target_var = data['lang']\n",
    "    return feature_matrix, target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 59) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# generate feature matrix for test and train data\n",
    "ngrams = get_feature_names(data_train, 4, 50)\n",
    "train_X, train_Y = get_feature_matrix(data_train, 4, ngrams)\n",
    "test_X, test_Y = get_feature_matrix(data_test, 4, ngrams)\n",
    "print(test_X.shape, test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of a language\n",
    "def langToIndex(lang):\n",
    "    return int(LANG.index(lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line))\n",
    "    for li, lang in enumerate(line):\n",
    "        tensor[li] = langToIndex(lang)\n",
    "        tensor = tensor.long()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of ngrams corresponds to the number of cols in the feature matrix minus 1 for the target variable\n",
    "DIM_INPUT = train_X.shape[1]\n",
    "DIM_OUTPUT = len(LANG)\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "L_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(DIM_INPUT, DIM_INPUT)\n",
    "        self.fc2 = nn.Linear(DIM_INPUT, DIM_INPUT)\n",
    "        self.fc3 = nn.Linear(DIM_INPUT, DIM_OUTPUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, criterion):\n",
    "    #model.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=L_RATE)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "tensor_x = torch.tensor(train_X.values.astype(np.float32))\n",
    "tensor_y = lineToTensor(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "<ipython-input-101-79b58313e02c>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1.0343570709228516\n",
      "19 0.9805341958999634\n",
      "29 0.9593043327331543\n",
      "39 0.952098548412323\n",
      "49 0.9493088722229004\n",
      "59 0.9479894638061523\n",
      "69 0.9472411870956421\n",
      "79 0.946749746799469\n",
      "89 0.9463869333267212\n",
      "99 0.9461105465888977\n",
      "109 0.9458827972412109\n",
      "119 0.9456905126571655\n",
      "129 0.9455293416976929\n",
      "139 0.9453927874565125\n",
      "149 0.9452614784240723\n",
      "159 0.9451424479484558\n",
      "169 0.9450288414955139\n",
      "179 0.9449203014373779\n",
      "189 0.9448147416114807\n",
      "199 0.9447058439254761\n",
      "209 0.9445931911468506\n",
      "219 0.9444803595542908\n",
      "229 0.9443602561950684\n",
      "239 0.9442311525344849\n",
      "249 0.9440850019454956\n",
      "259 0.9439287185668945\n",
      "269 0.9437574744224548\n",
      "279 0.9435518980026245\n",
      "289 0.9433176517486572\n",
      "299 0.9430469274520874\n",
      "309 0.942733645439148\n",
      "319 0.9423544406890869\n",
      "329 0.9418937563896179\n",
      "339 0.9413370490074158\n",
      "349 0.9406454563140869\n",
      "359 0.9397801160812378\n",
      "369 0.9386893510818481\n",
      "379 0.9372863173484802\n",
      "389 0.9354479312896729\n",
      "399 0.9330137372016907\n",
      "409 0.929719090461731\n",
      "419 0.9251969456672668\n",
      "429 0.9189499616622925\n",
      "439 0.9103779792785645\n",
      "449 0.8989794254302979\n",
      "459 0.884781539440155\n",
      "469 0.86859130859375\n",
      "479 0.8518424034118652\n",
      "489 0.8360244035720825\n",
      "499 0.8220821022987366\n",
      "509 0.8102287650108337\n",
      "519 0.800322949886322\n",
      "529 0.792066216468811\n",
      "539 0.7851162552833557\n",
      "549 0.7791748046875\n",
      "559 0.7739934921264648\n",
      "569 0.7693731784820557\n",
      "579 0.7650873064994812\n",
      "589 0.7608066201210022\n",
      "599 0.7561046481132507\n",
      "609 0.750510036945343\n",
      "619 0.7438616752624512\n",
      "629 0.736605703830719\n",
      "639 0.7289037108421326\n",
      "649 0.7208116054534912\n",
      "659 0.7124094367027283\n",
      "669 0.703734815120697\n",
      "679 0.6948679685592651\n",
      "689 0.6859885454177856\n",
      "699 0.6773503422737122\n",
      "709 0.6692138910293579\n",
      "719 0.6617828011512756\n",
      "729 0.6551762223243713\n",
      "739 0.6494134664535522\n",
      "749 0.6444582939147949\n",
      "759 0.6402180194854736\n",
      "769 0.6366017460823059\n",
      "779 0.6335117220878601\n",
      "789 0.6308576464653015\n",
      "799 0.6285607814788818\n",
      "809 0.6265732645988464\n",
      "819 0.6248263120651245\n",
      "829 0.6232858300209045\n",
      "839 0.6219212412834167\n",
      "849 0.6206954717636108\n",
      "859 0.6195932030677795\n",
      "869 0.6185908913612366\n",
      "879 0.6176819205284119\n",
      "889 0.6168502569198608\n",
      "899 0.6160930395126343\n",
      "909 0.6153966784477234\n",
      "919 0.6147445440292358\n",
      "929 0.6141351461410522\n",
      "939 0.6135866045951843\n",
      "949 0.6130878925323486\n",
      "959 0.6126319169998169\n",
      "969 0.6122108697891235\n",
      "979 0.6118168234825134\n",
      "989 0.6114476919174194\n",
      "999 0.6110999584197998\n"
     ]
    }
   ],
   "source": [
    "for e in range(N_EPOCHS):\n",
    "    loss, output = train(net, tensor_x, tensor_y, opt, crit)\n",
    "    if e % 10 == 9:\n",
    "        print(e, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-79b58313e02c>:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.fc3(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(tensor_x.shape[0]):\n",
    "        inp = tensor_x[i]\n",
    "        ground_truth = tensor_y[i]\n",
    "        outputs = net(inp)\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        total += 1\n",
    "        correct += (predicted == ground_truth).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
